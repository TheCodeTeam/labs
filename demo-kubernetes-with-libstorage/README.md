# Exploring Kubernetes with libStorage persistent volumes on AWS - the Hard WayThe [libStorage](http://libstorage.readthedocs.io/en/stable/) integration with [Kubernetes](http://kubernetes.io/) has been submitted by the [{code} team](https://codedellemc.com/) as a [Pull Request](https://github.com/kubernetes/kubernetes/pull/28599) for consideration by the Kubernetes project.Until this is merged into the Kubernetes tree, this procedure can be utilized to evaluate Kubernetes using libStorage enabled external volume mount support.This document aspires to be an homage to [@kelseyhightower](https://twitter.com/kelseyhightower) â€˜s brilliant Kubernetes the Hard Way [project](https://github.com/kelseyhightower/kubernetes-the-hard-way). It's not a model for production deployment. This tutorial takes a long-ish route, to help people understand all the components and configuration involved, using a popular platform that is accessible to many organizations. We are going to build Kubernetes from source, manually installing the build tools and the libStorage component. The steps could have been hidden in a "black box" behind an automated orchestration tool. This exercise should take an hour or less. Kubernetes and libStorage can run on many cloud and physical platforms, with multiple host OS options, supporting a large and growing number of storage platforms. The libStorage allows automation to deal with the combinatorial explosion of options. Other non-AWS choices of platforms and storage are operationally identical to what is demonstrated here. In fact, libStorage is designed to make it feasible to add, swap or migrate workloads and providers at many levels.### Prerequisites- An Amazon AWS account with:    - an ssh key pair for remote login    -  An Access Key for API access        - Access Key ID        - Secret Access Key ### Step 1: Deploy a VM using an AMI1. Choose EC2 from the AWS console dashboard, an Launch an instance.2. We will use the **Ubuntu Server 16.04 LTS (HVM), SSD Volume Type**. 64-bit. The ami number varies by region. A t2.large instance type is recommended because the Kubernetes build we will be performing can utilize over 4GB of memory. (An m4.large could also be utilized.)3. Configure instance details to set the Root volume to 40GB. Other settings can be left as default.4. Select your ssh key and launch.5. You can name your instance 'kubernetes-libstorage' while it initializes.### Step 2: Install Go in the VMLogin to the VM using the tool of your choice. The AMI's user and password will be 'ubuntu'.Execute these commands:```cd ~curl https://storage.googleapis.com/golang/go1.6.3.linux-amd64.tar.gz | tar xvzsudo chown -R root:root ./go/ sudo mv go /usr/localecho "export GOPATH=$HOME/work" >> .profile echo "export PATH=$PATH:/usr/local/go/bin:$HOME/work/bin" >> .profilesource .profile```### Step 3: Install Docker```sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609Decho "deb https://apt.dockerproject.org/repo ubuntu-xenial main" | sudo tee /etc/apt/sources.list.d/docker.listsudo apt-get updatesudo apt-get install -y docker-engine```### Step 4: Install etcd```curl -L https://github.com/coreos/etcd/releases/download/v3.0.13/etcd-v3.0.13-linux-amd64.tar.gz | tar xvzsudo chown -R root:root ./etcd-v3.0.13-linux-amd64/sudo mv etcd-v3.0.13-linux-amd64/ /usr/local/sudo ln -s /usr/local/etcd-v3.0.13-linux-amd64/etcd /usr/local/bin/etcdsudo ln -s /usr/local/etcd-v3.0.13-linux-amd64/etcdctl /usr/local/bin/etcdctl```### Step 5: Install build tools: g++, make, mercurial, unzip, ```sudo apt-get install -y g++ make mercurial  unzipgo get -u github.com/tools/godepgo get -u github.com/jteeuwen/go-bindata/go-bindata```### Step 6: Checkout the libStorage fork of Kubernetes from github```mkdir -p $GOPATH/src/k8s.io && cd $GOPATH/src/k8s.iogit clone https://github.com/vladimirvivien/kubernetes.gitcd kubernetesgit remote add upstream https://github.com/kubernetes/kubernetes.gitgit checkout -b libstorage-take4 origin/libstorage-take4```### Step 7: Install REX-Ray[REX-Ray](http://rexray.readthedocs.io/en/stable/) is a vehicle for installing prepackaged a prepackaged instance of a libStorage server (or lightweight client with CLI). This server centralizes and abstracts the consumption of external persistent storage. In a large scale production cluster, REX-Ray would be run as a central service, within a container, using the high availability features of the orchestrator platform. Because we have a single node Kubernetes installation in this exercise, it happens to be on the single cluster node. ```curl -sSL https://dl.bintray.com/emccode/rexray/install | sh -```As sudo, create the file /etc/rexray/config.yml with this content using your favorite editor, and substituting your AWS details (two keys and region):```rexray:  loglevel: warn  modules:    default-docker:      disabled: truelibstorage:  logging:    level: debug  host: tcp://127.0.0.1:7979  embedded: true  service: ebs  server:    endpoints:      public:        address: tcp://127.0.0.1:7979    services:      ebs:        driver: ebsebs:  accessKey: <my-access-key-here>  secretKey: <my-secret-key-here>  region:    <my-aws-region>  maxRetries: 7````sudo rexray service start`### Step 8: Build and run Kubernetes```sudo susource /home/ubuntu/.profileexport KUBERNETES_PROVIDER=localcd /home/ubuntu/work/src/k8s.io/kubernetes hack/local-up-cluster.sh ```The build should take about 10 minutes using the recommended large instance type. Warning: build failures on a host with inadequate memory can result in errors not obviously related to memory. ### Step 9: Start another session and run a few command to verify operation of the newly built and deployed Kubernetes instance```export KUBERNETES_PROVIDER=localcd work/src/k8s.io/kubernetes/cluster/kubectl.sh config set-cluster local --server=http://127.0.0.1:8080 --insecure-skip-tls-verify=truecluster/kubectl.sh config set-context local --cluster=localcluster/kubectl.sh config use-context localcluster/kubectl.sh get nodescluster/kubectl.sh get podscluster/kubectl.sh get servicescluster/kubectl.sh get deployments```### Step 10: Create a PersistentVolume and utilize it in a podCreate an ebs volume like this:`rexray volume create mysql-1 --size=1`This should create the result shown below, and creation of the specified volume can be confirmed using the ELASTIC BLOCK STORE Volumes section of the AWS console.```ubuntu@ip-172-31-37-235:~/work/src/k8s.io/kubernetes$ rexray volume create vol-0001 --size=1ID            Name      Status     Sizevol-72a2eec6  mysql-1  available  1```Create the file pod.yaml with this content:```apiVersion: v1kind: Podmetadata:  name: mysql  labels:    name: mysqlspec:  containers:  - image: mysql:5.6    name: mysql    env:      - name: MYSQL_ROOT_PASSWORD        # CHANGE THIS PASSWORD!        value: yourpassword    ports:      - containerPort: 3306        name: mysql    volumeMounts:      # This name must match the volumes name below.    - name: mysql-persistent-volume      mountPath: /var/lib/mysql  volumes:  - name: mysql-persistent-volume    libStorage:      host: http://127.0.0.1:7979      # This disk must already exist      volumeName: mysql-1      service: ebs````cluster/kubectl.sh create -f pod.yaml``cluster/kubectl.sh describe pod`Use the AWS console to verify that the volume was created and that it is shown as "in-use" indicating it is attached to a VM### Step 11: Create a PersistentVolumeClaim and utilize itUsers of Kubernetes can request persistent storage for their pods. Administrators can utilize *claims* to allow the storage provisioning and lifecycle to be maintained independently of the applications consuming storage. A background process satisfies claims from an inventory of *persistent volumes*. 

Persistent volumes are intended for "network volumes" like GCE persistent disks, and AWS elastic block stores, but can also represent volumes from "overlay" storage providers like ScaleIO. A Persistent Volume (PV) in Kubernetes represents a real piece of underlying storage capacity in the infrastructure. 

A user (application) sees and consumes claims, which hide storage implementation details. This allows an application's configuration to be portable across platforms and providers.
Create an AWS ebs volume:`rexray volume create admin-managed-1g-01 --size=1`Create a Kubernetes persistent volume referencing it:Create the file pv.yaml with this content:```kind: PersistentVolumeapiVersion: v1metadata:  name: small-pv-pool-01spec:  capacity:    storage: 1Gi  accessModes:    - ReadWriteOnce  libStorage:    host: http://127.0.0.1:7979    service: ebs    volumeName: admin-managed-1g-01````cluster/kubectl.sh create -f pv.yaml`Create a Kubernetes persistent volume claim:Create the file pvc.yaml with this content:```kind: PersistentVolumeClaimapiVersion: v1metadata:  name: pg-data-claimspec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 1Gi````cluster/kubectl.sh create -f pvc.yaml`Create a pod that uses the claim:Create the file pod-pvc.yaml with this content:```kind: PodapiVersion: v1metadata:  name: postgres  labels:    name: postgresspec:  containers:    - name: postgres      image: postgres      env:        - name: POSTGRES_PASSWORD          # CHANGE THIS PASSWORD!          value: mysecretpassword        - name: PGDATA          value: /var/lib/postgresql/data/pgdata      ports:      - containerPort: 5432        name: postgres      volumeMounts:      - mountPath: /var/lib/postgresql/data        name: pg-data  volumes:    - name: pg-data      persistentVolumeClaim:        claimName: pg-data-claim````cluster/kubectl.sh create -f pod-pvc.yaml`### Step 12: Utilize dynamic provisioning by storage class
Dynamic provisioning is a mechanism that allows storage volumes to be created on demand, rather than pre-provisioned by an administrator. This is a Beta feature of Kubernetes 1.4.

Create a storage class:Create the file sc.yaml with this content:```kind: StorageClassapiVersion: storage.k8s.io/v1beta1metadata:  name: bronze  labels:    name: bronzeprovisioner: kubernetes.io/libstorageparameters:  host: http://127.0.0.1:7979  service: ebs````cluster/kubectl.sh create -f sc.yaml`Create a persistent volume claim that utilizes the storage class:Create the file sc-pvc.yaml with this content:```kind: PersistentVolumeClaimapiVersion: v1metadata:  name: pvc-0002  annotations:      volume.beta.kubernetes.io/storage-class: bronzespec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 1Gi````cluster/kubectl.sh create -f sc-pvc.yaml`Create a pod that uses the persistent volume claim:Create the file pod-sc-pvc.yaml with this content:```kind: PodapiVersion: v1metadata:  name: podscpvc-0001spec:  containers:    - name: webserv      image: gcr.io/google_containers/test-webserver      volumeMounts:      - mountPath: /test        name: test-data  volumes:    - name: test-data      persistentVolumeClaim:        claimName: pvc-0002````cluster/kubectl.sh create -f pod-sc-pvc.yaml`## ContributionCreate a fork of the project into your own repository. Make all your necessary changes and create a pull request with a description on what was added or removed and details explaining the changes in lines of code. If approved, project owners will merge it.## SupportPlease file bugs and issues on the GitHub issues page for this project. This is to help keep track and document everything related to this repo. For general discussions and further support you can join the [{code} by Dell EMC Community slack channel](http://community.codedellemc.com/). The code and documentation are released with no warranties or SLAs and are intended to be supported through a community driven process.